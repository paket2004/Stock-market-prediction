{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RbelOPK4TFB"
      },
      "source": [
        "# Business and data understanding\n",
        "------------\n",
        "\n",
        "The initial phase is concerned with tasks to define the business objectives and translate it to ML objectives, to collect and verify the data quality and to finaly assess the project feasibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZZ0T5RTaQon"
      },
      "source": [
        "![](https://i.imgur.com/55J7fBc.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "516PRaCCaQoo"
      },
      "source": [
        "## Terminology\n",
        "\n",
        "----------\n",
        "\n",
        "### The tasks\n",
        "\n",
        "Compile a glossary of terminology relevant to the project. This may include two components:\n",
        "(1) A glossary of relevant business terminology, which forms part of the business understanding available to the project. Constructing this glossary is a useful \"knowledge elicitation\" and education exercise.\n",
        "(2) A glossary of machine learning terminology, illustrated with examples relevant to the business problem in question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHJsmcqkq10"
      },
      "source": [
        "### The output\n",
        "\n",
        "**Business terminology:**\n",
        "\n",
        "* **Dividend Yield**: A financial ratio that shows how much a company pays out in dividends each year relative to its stock price.\n",
        "\n",
        "* **Earnings Per Share (EPS)**: The portion of a company's profit allocated to each outstanding share of common stock.\n",
        "\n",
        "* **Market Capitalization**: The total market value of a company's outstanding shares, calculated as stock price multiplied by the total number of shares outstanding.\n",
        "\n",
        "* **Sentiment Analysis**: The process of computationally identifying and categorizing opinions expressed in text to determine whether the writer's attitude towards a particular topic is positive, negative, or neutral.\n",
        "\n",
        "* **Arbitrage**: The simultaneous purchase and sale of the same or equivalent assets in different markets to profit from price differences.\n",
        "\n",
        "* **Quantitative Easing (QE)**: A monetary policy whereby a central bank buys government securities or other securities from the market to increase the money supply and encourage lending and investment.\n",
        "\n",
        "* **Volatility**: The degree of variation of a trading price series over time, usually measured by the standard deviation of returns.\n",
        "\n",
        "* **Sentiment Analysis**: The process of computationally identifying and categorizing opinions expressed in text to determine whether the writer's attitude towards a particular topic is positive, negative, or neutral.\n",
        "\n",
        "* **The Global Industry Classification Standard (GICS)**: method for assigning companies to a specific economic sector and industry group that best defines its business operations.\n",
        "\n",
        "* **Bull Market**: A financial market in which prices are rising or are expected to rise.\n",
        "\n",
        "**ML-terminology:**\n",
        "\n",
        "* **Feature:** An individual measurable property or characteristic of a phenomenon being observed, such as stock price, volume, or sentiment score.\n",
        "\n",
        "*  **Label**: The outcome variable that is being predicted or classified, such as stock price movement or sentiment classification.\n",
        "\n",
        "* **Training Set:** A subset of the dataset used to train machine learning models.\n",
        "\n",
        "* **Test Set**: A subset of the dataset used to evaluate the performance of trained machine learning models.\n",
        "\n",
        "* **Validation Set**: A subset of the dataset used to tune the hyperparameters of the model.\n",
        "\n",
        "* **Regression:** The task of predicting a continuous value, such as the future stock price.\n",
        "\n",
        "* **Exploratory data analysis (EDA)**: approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.\n",
        "\n",
        "* **Model:** A mathematical representation of a process, built by training on data.\n",
        "\n",
        "* **ML inference:** process of running data points into a machine learning model to calculate an output such as a single numerical score.\n",
        "\n",
        "* **Overfitting**: When a model learns the details and noise in the training data to the extent that it performs poorly on new data.\n",
        "\n",
        "In case of more business terminology appearing during the data explanatory analysis and model building all definitions and explanations would be provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfrdzmhe0Zfr"
      },
      "source": [
        "## Scope of the project\n",
        "----------\n",
        "\n",
        "### The tasks\n",
        "- Explore the background of the business.\n",
        "- Define business problem\n",
        "- Define business objectives\n",
        "- Translate business objectives into ML objectives\n",
        "\n",
        "The objective here is to thoroughly understand, from a business perspective, what the client really wants to accomplish. Often the client has many competing objectives and constraints that must be properly balanced. The goal is to uncover important factors, at the beginning, that can influence the outcome of the project. A possible consequence of neglecting this step is to expend a great deal of effort producing the right answers to the wrong questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7dy9AhaYsQ"
      },
      "source": [
        "### The output\n",
        "\n",
        "#### 1. Background\n",
        "  >X is an investment bank. They offer a wide range of services, including investment banking, securities trading, wealth management, and asset management. Despite their strong market position, X is currently struggling with the volatility and unpredictability of financial markets.\n",
        "\n",
        "#### 2. Business problem\n",
        "  >X Investment Group is facing significant challenges in navigating the increasingly volatile and unpredictable financial markets. The firm needs to enhance its predictive accuracy for stock prices to optimize trading strategies and mitigate risks. Traditional models have proven inadequate in processing and analyzing the vast amounts of real-time data generated by the markets, leading to suboptimal trading decisions and missed opportunities. To maintain its competitive edge and improve financial performance, X requires a sophisticated machine learning model that can accurately predict closing stock prices and provide actionable insights for better decision-making.\n",
        "\n",
        "#### 3. Business objectives\n",
        "  >What is the impact of market sentiment on stock prices?\n",
        "  \n",
        "  >How do corporate earnings announcements affect stock prices?\n",
        "\n",
        "#### 4.ML objectives\n",
        "  > Predict the closing prices of stocks using historical market data, trade volumes, and relevant news data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRCq_z_raQop"
      },
      "source": [
        "## Success Criteria\n",
        "-------------\n",
        "\n",
        "### The tasks\n",
        "- Describe the success criteria of the ML project on three different levels: the business success criteria, the ML success criteria and the economic success criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIHAyj3AmbgW"
      },
      "source": [
        "### The output. Success Criteria\n",
        "#### 1.Business success criteria\n",
        "- Increase user engagement and satisfaction by 10% within the next 6 months\n",
        "- Improve investment performance and outperform the market by 2% within a year\n",
        "\n",
        "\n",
        "#### 2. ML success criteria\n",
        "- Improve the prediction accuracy so that the error was less than 30%\n",
        "- Achieve a predictive precision of at least 70% for daily stock price predictions.\n",
        "\n",
        "#### 3. Economic success criteria\n",
        "- Increase the owners' profit by 5% within a month and make the model deliver a positive return on investment\n",
        "- Deliver the operational efficiency and integrate the model into existing workflows without significant delays\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuRcECj0aQoq"
      },
      "source": [
        "## Data collection\n",
        "----------\n",
        "\n",
        "### The tasks\n",
        "- Specify the data sources\n",
        "- Collect the data\n",
        "- Version control on the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCYtXr7Me9V5"
      },
      "source": [
        "### The output\n",
        "\n",
        "#### Data collection report:\n",
        "\n",
        "**Data Source:** Stock-NewsEventsSentiment (SNES) 1.0â€Š is a dataset consisting of market and news time series data for S&P 500 companies over a period of 21 months that X was collecting from October 2020 to July 2022.\n",
        "    \n",
        "**Data Type:** The data consists of numerical values, categorical data, and date. Numerical values represent the price of the stock during the day (Open, High, Low, Close). Date represents the date of the deal and categorical values represents industry sector.\n",
        "\n",
        "**Data Size:** The dataset contains more than 200 hundred thousands of records with 27 features each.\n",
        "\n",
        "**Data Collection Method:** The data collection involved aggregating news articles from various financial news sources and natural language processing techniques were employed to extract sentiment from the articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUo1H0U0huZL"
      },
      "source": [
        "#### Data version control report:\n",
        "\n",
        "**Data Version:** \"The current data version is v1.1, which was updated on june 21, 2024.\"\n",
        "\n",
        "**Data Change Log:** The data change log shows that the date of the deal feature was updated on June 21, 2024, to encode time as categorical feature.\n",
        "\n",
        "**Data Backup:** The company has a daily backup of the data stored on a PC.\n",
        "\n",
        "**Data Archiving:** The company archives data older than five years to a cloud storage service for long-term retention.\n",
        "\n",
        "**Data Access Control:** The company uses role-based access control to ensure that only developers and employees can access data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1RoNg5PaQoq"
      },
      "source": [
        "## Data quality verification\n",
        "--------\n",
        "\n",
        "### The tasks\n",
        "- Describe data\n",
        "- Define data requirements\n",
        "- Explore the data\n",
        "- Verify the data quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Bqj3FKjliw"
      },
      "source": [
        "### The output\n",
        "\n",
        "#### 1. Data description\n",
        "\n",
        " The data acquired for this project includes a dataset of more than 40000 (without splitting on batches for data updating simulation) records with 27 fields each. The fields include date of the deal, stock prices during the day (High, Low, Open, Close), GISC sector of the stock, and articles data. The data is in a CSV format and is stored in a local database.\n",
        "\n",
        "The table:\n",
        "\n",
        "| The column name        | Description                                                                 |\n",
        "|-------------------|-----------------------------------------------------------------------------|\n",
        "| Date              | Date of the news event                                                      |\n",
        "| Open              | The openning price                                                          |\n",
        "| High              | The highest price of the stock during a day                                 |\n",
        "| Low               | The lowest price during a day                                               |\n",
        "| Close             | The price to the end of a day (the closing price)|\n",
        "| Adj_close| The adjusted closing price|\n",
        "| Volume | The number of shares |\n",
        "| Symbol | The stock ticker symbol |\n",
        "| Security | The name of security |\n",
        "| GICS Sector | The Global Industry Classification Standard sector |\n",
        "| GICS Sub-Industry | The GICS sub-industry |\n",
        "\n",
        "After those columns the dataset contains features about news with corresponding themes: all news volume, volume, positive sentiment, negative sentiment, new products, layoffs, analyst comments, stocks, dividends, corporate earnings, merges & acquisitions, store openings, product recalls, adverse events, personnel changes, stock rumors. All the columns are in numerical format (float64).\n",
        "\n",
        "\n",
        "#### 2. Data Exploration\n",
        "An interesting finding from the initial data exploration is the positive correlation between the closing price (\"Close\") and the opening price (\"Open\"), the lowest price (\"Low\"), and the highest price (\"High\") of the day. This suggests that stocks with higher opening prices tend to also have higher closing prices, lows, and highs throughout the trading day. Conversely, stocks that open lower tend to see their price stay within a lower range for the day. This initial observation highlights a relationship between a stock's starting point and its overall price movement during a trading session. We built a heatmap for the first 100 entries in the dataset to demonastrate the correlation:\n",
        "\n",
        "![Correlation](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/correlation.jpg)\n",
        "![Correlation](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/correlation2.jpg)\n",
        "![Correlation](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/dependencies.jpg)\n",
        "\n",
        "However, the closing price forming includes other features too. Our goal is to decrease the percentage of the error, thus we must take into account all the features and do not rely only on existing prices, probably assigning more weights to news or GICS values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5WgKwV6qctZ"
      },
      "source": [
        "#### 3. Data requirements\n",
        "\n",
        "\n",
        "\n",
        "*   Date should be stored in a format year-month-day.\n",
        "*   Open. High, Low, Close price should be float numbers (>0)\n",
        "*   Every feature column should not contain more than 5% of missing values\n",
        "*   Data should contain numerical, categorical and time features that will be converted or encoded\n",
        "*   Data should be stored in csv format\n",
        "*   GICS sector is a categorical text data and should correspond to valid sector, for example: 'Energy', 'Materials', 'Industrials'.\n",
        "*   News headline is a text data and amount of characters is bounded from 10 to 500.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTre2a3tt1-x"
      },
      "source": [
        "#### 4. Data quality verification report\n",
        "\n",
        "**Completeness:** The data is complete in the sense that it covers all the required cases. It contains necessary fields, doesn't exceed the available amount of missing values, and desired data type and limitations are followed in this dataset\n",
        "\n",
        "**Correctness:** The data appears to be correct, with no obvious errors. However, a manual review is needed to check some typos.\n",
        "\n",
        "**Missing Values:** There are missing values in the data. However, their amount is not significant. Missing values filling technique were applied to avoid this issue.\n",
        "\n",
        "Overall, the data quality is high, and the data is suitable for analysis and modeling. Our team is sure, that we spent adequate amount of efforts to evaluate this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZCQA0YfF1Yq"
      },
      "source": [
        "## Project feasibility\n",
        "-------------\n",
        "This task involves more detailed fact-finding about all of the resources,constraints, assumptions and other factors that should be considered in determining the data analysis goal and project plan. In the previous task, your objective is to quickly get to the crux of the situation. Here, you want to flesh out the details.\n",
        "\n",
        "### tasks\n",
        "- Assess the project feasibility\n",
        "- Create POC (Proof-of-concept) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZG39mqO3gOp"
      },
      "source": [
        "### Output\n",
        "#### 1. Inventory of resources\n",
        "\n",
        "**PERSONNEL:**\n",
        "\n",
        ">Data Experts: Data scientists and analysts skilled in data cleaning, preprocessing, and feature engineering.\n",
        "\n",
        ">Machine Learning Personnel: Data scientists and machine learning engineers experienced in building a models suitable for the corresponding task.\n",
        "\n",
        ">Business experts: Individuals with domain knowledge in finance and stock market trends.\n",
        "\n",
        "**DATA:**\n",
        "\n",
        ">Fixed Extracts: Access to the \"Stock News Events Sentiment (SNES-10)\" dataset (5 different versions).\n",
        "\n",
        "**COMPUTING RESOURCES:**\n",
        "\n",
        ">Hardware Platforms: High-performance servers or cloud computing resources, and local machine resources for data processing and model training.\n",
        "\n",
        ">Storage: Sufficient storage for datasets and backups on local machines.\n",
        "\n",
        ">Software: ML libraries documentstion, Google collab for the team work.\n",
        "\n",
        "#### 2. Requirements, assumptions and constraints\n",
        "\n",
        "_The project timeline_:\n",
        "\n",
        "Phase I - Business and data understanding - week 1\n",
        "\n",
        "Phase II - Data engineering/preparation - week 2\n",
        "\n",
        "Phase III - Model engineering - week 3\n",
        "\n",
        "Phase IV - Model validation - week 4\n",
        "\n",
        "Phase V - Model deployment - week 5\n",
        "\n",
        "Phase VI - Model monitoring and maintenance - week 6\n",
        "\n",
        "**Comprehensibility and quality of results:** Clear, interpretable results and high-quality, accurate sentiment analysis.\n",
        "\n",
        "**Security and legal issues:** For avoiding data leaks we have to ensure that the data is used safely, it is not stored within open source, and our actions correspond to company policies and legal regulations.\n",
        "\n",
        "**Data usage permission:** Confirm permission to use the \"Stock News Events Sentiment (SNES-10)\" dataset for analysis and model development. To analyse the gotten conditions for using the data.  \n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        ">Data quality: The dataset is (assumed to be) of high quality with minimal missing or erroneous values (Data Quality block).\n",
        "\n",
        ">Relevance of sentiment: Consider the sentiment analysis as an _important_ part in the Close price (target) forming to avoid relying on \"High\" and \"Low\" prices mostly so that we could get more accurate results and improve the predictions.\n",
        "\n",
        ">Business conditions: Assume that the financial market conditions remain relatively stable during the project duration.\n",
        "\n",
        "**Constraints:**\n",
        "\n",
        ">Resource availability: Limited availability of personnel and computing resources, the lack of data.\n",
        "\n",
        ">Data size: Practical constraints on the size of data that can be processed and modeled effectively within the available computing resources. The highly overloaded database might be a bottleneck in model training, thus we need to find a trade-off between underfitting (small data) and time consuming training (big data).\n",
        "\n",
        ">Technological constraints: Limitations related to software compatibility and integration with existing systems. Too complex and 'heavy' model might be too slow or inapropriate for intagrating it to alredy built processes.\n",
        "\n",
        "#### 3. Risks and Contingencies\n",
        "\n",
        "**Risks:**\n",
        "\n",
        ">Data Issues: Incomplete, incorrect, or irrelevant data can lead to inaccurate models. There can be no data leack. The lack or excess of data.\n",
        "\n",
        ">Resource Limitations: Insufficient computational resources or personnel availability.\n",
        "\n",
        ">Technological Failures: Hardware or software malfunctions during critical phases of the project. The wrong choice of model, underfitting/overfitting, low precision.\n",
        "\n",
        ">Regulatory Changes: New regulations that restrict data usage or access (failure of training and duty of finding a new dataset for project implementation).\n",
        "\n",
        "**Contingencies:**\n",
        "\n",
        ">Data Issues: Implement data validation checks and cleaning procedures. Source additional data if necessary.\n",
        "\n",
        ">Resource Limitations: Prioritize tasks and consider cloud-based solutions to scale resources dynamically.\n",
        "\n",
        ">Technological Failures: Regular backups and use of reliable infrastructure with redundancy.\n",
        "\n",
        ">Regulatory Changes: Stay informed about regulatory updates and adjust data usage practices accordingly.\n",
        "\n",
        "#### 4. Costs and Benefits\n",
        "\n",
        "**Costs:**\n",
        "\n",
        ">Computing Resources: Costs of servers for model training.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        ">Enhanced Decision-Making: Improved sentiment analysis can lead to better investment decisions and increase of money earning.\n",
        "\n",
        ">Competitive Advantage: Early identification of market trends through sentiment analysis.\n",
        "\n",
        ">Cost Savings: Efficient data preprocessing and analysis can reduce the need for manual analysis. For example we must get rid of zero values that cannot influence the results, encode categorical features with apropriate encoders, keep the data clean, and get rid of inefficient features.\n",
        "\n",
        ">Scalability: Development of a scalable model that can be adapted for real-time sentiment analysis or integrated into a real systems.\n",
        "\n",
        "**Cost-Benefit Analysis:**\n",
        "\n",
        "![Cost-Benefit_Table](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/costbenefir.jpg)\n",
        "\n",
        "\n",
        "We evaluate this project in 36640$ (check the next point - proof of concept) and as a perspective and encouraging one and believe in its successful implementation. We take into account unpredictable situations and sure we would be able to deal with them in case of any. An accurate model worths the work with data, model building, and automating the processes, thus we evaluate it as a profitable and exciting theme. The total cost consists of server rent, working hours and additional resource wasting for stressful situations.\n",
        "\n",
        "**Potential Benefits:**\n",
        "\n",
        "Increased revenue through better investment decisions: \\$500,000 annually.\n",
        "Operational efficiency savings: $50,000 annually.\n",
        "Intangible benefits like enhanced reputation and market intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HMGTreM8Yw"
      },
      "source": [
        "#### 5. Feasibility report. Proof of Concept model\n",
        "\n",
        "The plan is taken from the provided in the chat link: https://habr.com/ru/companies/ods/articles/438212/\n",
        "\n",
        "ROI (Return of investment) - an indicator of the profitability of the project, equal to the ratio of income to investments spent.\n",
        "\n",
        "\n",
        "##### 1) Desires.\n",
        "Bank X is experiencing challenges in identifying banks at risk of closure. Early identification allows for proactive measures to be taken, such as mergers, acquisitions, or restructuring. Currently, bank closure prediction relies on manual analysis by financial experts, which is time-consuming and subjective. Thus they need an automated predicted 'Close' values.\n",
        "\n",
        "Our task: develop a machine learning model to automate bank close prices prediction.\n",
        "\n",
        "Input: readily available financial data points (already present in the provided dataset) for prediction\n",
        "\n",
        "Output: generated values of Close prices\n",
        "\n",
        "##### 2) Experiment.\n",
        "Business process: Close prices prediction and prediction automation\n",
        "\n",
        "ML task: regression\n",
        "\n",
        "Training data for the experiment: some 300 records, feature-vector building for each of them.\n",
        "\n",
        "Technical metrics: MSE, RMSE\n",
        "\n",
        "Business metrics: prediction accurace in percentage format (the difference between predicted and actual values in percents)\n",
        "\n",
        "##### 3) Data.\n",
        "The data will be taken from the provided data from the bank X. They kept records every day and were ready to share with us with observations. Since its a real data, we have no need to check if the data from the training dataset wolud be different from the real-world data. We assume that the dataset is well-structured, deep, and consistent.\n",
        "\n",
        "##### 4) Model building.\n",
        "Feature engineering: prepare the dataset, encode categorical variables, drop some redundant columns.\n",
        "\n",
        "Model selection: choose the simplest regression model - regression model.\n",
        "\n",
        "Model training: train the model on the prepared data, splitting it into training and testing sets.\n",
        "\n",
        "##### 5) Model evaluation\n",
        "Cross-validation: perform the cross-validation to avoid overfitting.\n",
        "\n",
        "Metrics assessment: calculate technical metrics (MSE, RMSE) and business metrics (average prediction accuracy within a defined price range) on the testing set.\n",
        "\n",
        "##### 6) ROI evaluation\n",
        "Since the bank is in charge of data collection, so our model can easily get new data for prediction of new values. However, we take risks that there might be problems with it and some predictions might be less effective and beneficial.\n",
        "\n",
        "Costs:\n",
        "\n",
        "Development Costs:\n",
        "    Model training and optimization.\n",
        "    Integration development with existing systems.\n",
        "    Automation development for seamless operation.\n",
        "Operational Costs:\n",
        "    Infrastructure upkeep for model deployment.\n",
        "    Model retraining at defined intervals to maintain accuracy.\n",
        "    Data access fees or subscription costs (if applicable).\n",
        "\n",
        "Calculating existing costs:\n",
        "\n",
        "Assume the Close manual calculation takes 3 hours, server maintainance 2 hours, and data tracking 2 hours. Current analists waist 7 hours on Close price prediction. The whole process costs the company 300$ per day.  \n",
        "\n",
        "Calculating the project costs:\n",
        "\n",
        "Assume the project takes 8 hours per week for each team member. There are 6 weeks, so the working time is: 144 hours. The hour salary rate is 60\\$, so the project cost is 8640\\$. Adding the maintainance cost (3000\\$ per month (server, manual work and time)).\n",
        "\n",
        "ROI calculation:\n",
        "\n",
        "For the next 6 months the company waists 300 * 30 * 6 = 54000\\$\n",
        "\n",
        "After the model integration the company pays: 8640+3000*6+10000 = 36640, where 8640 the project cost, 2000 per month is maintainance cost, and 10000 is for inpredictable situations (increased project duration or other).\n",
        "\n",
        "Additionally, the company gets the stability on financial markets.\n",
        "\n",
        "Thus, the ROI 54000/36640 = 1.47 and the payback period is less than 6 months.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv9AUIJEaQor"
      },
      "source": [
        "## Project plan\n",
        "----------------\n",
        "\n",
        "### task\n",
        "\n",
        "Describe the intended plan for achieving the machine learning goals and thereby achieving the business goals. The plan should specify the anticipated set of steps to be performed during the rest of the project including an initial selection of tools and techniques.\n",
        "\n",
        "### output\n",
        "\n",
        "#### 1. Project plan\n",
        "- List the stages to be executed in the project, together with their duration, resources required, inputs, outputs, and dependencies. Where possible, try and make explicit the large-scale iterations in the machine learning process, for example, repetitions of the modeling and evaluation phases. As part of the project plan, it is also important to analyze dependencies between time schedule and risks. Mark results of these analyses explicitly in the project plan, ideally with actions and recommendations if the risks are manifested. Decide at this point which evaluation strategy will be used in the evaluation phase. Your project plan will be a dynamic document. At the end of each phase youâ€™ll review progress and achievements and update the project plan accordingly. Specific review points for these updates should be part of the project plan.\n",
        "\n",
        "- Build a Gantt chart for the project tasks and phases using some online platforms like TeamGantt, jira, goodday, tello, ...etc\n",
        "\n",
        "- Add all of your team members and assign tasks to them preliminary. Then you check daily the progress.\n",
        "\n",
        "#### 2. ML project Canvas\n",
        "At the end of the first phase, you should create a canvas for the project as a summary of this phase.\n",
        "\n",
        "\n",
        "> ##### Example\n",
        "> Follow the link: https://github.com/louisdorard/machine-learning-canvas/blob/master/churn.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WECiJ2UM8Yw"
      },
      "source": [
        "### Output\n",
        "\n",
        "Stages:\n",
        "\n",
        "1) Data acquisition. Get the dataset, aggreements with data providers (bank X). Distribute the workload between team members, build the plan, define business problem. 1 week. Input: project idea. Output: found dataset. Resources: 4-5 working hours, personal computers, software (kaggle). Risks: refusal from the bank to use their data and their software. Solution: find another problem and agree with a new company.  \n",
        "\n",
        "2) Data preparation. Exploratory data analysis, data preprocessing and feature engineering for model applying. 1 week. Input: found dataset. Output: cleared and prepared dataset. Resources: 3-4 working time, personal computers and additional software (google collab, libraries documentation, etc.). Risks: too large or too small dataset. Solution: Reduce the dataset using an appropriate technique or find more data and combine it with existing one.\n",
        "\n",
        "3) Model building. Model selection and training. Input: prepared dataset, splitted on test and train parts. Output: trained model, ready for testing and probably for integration. Resources: 4-5 working hours, probably server usage. Risks: the hardware is not enough. Solution: use google collab GPU power or get a new server. 1 week.\n",
        "\n",
        "4) Model validation. Check the metrics using test data and evaluate it. Check if the gotten results are acceptable and suits the success criteria. Input: test data. Output: metrics and decision about model deployment. Process: analysis of gotten results and model tunning. Resources: 3-4 hours, hardware (server probably) and software. Risks: low accuracy. Soluition: check the prepared dataset, try to extract as much features as possible, tune the model. 1 week.\n",
        "\n",
        "5) Model deployment. Integrating the tunned model into esisting processes. Automate it for a new data appearance. Check for any inconsistiences. 1 week. Input: server and built model. Output: fully automated and integrated model. Resources: server, some working hours. Risks: problem with integrating, model is too big or slow. Solution: apply the optimization techniques.\n",
        "\n",
        "6) Model monitoring. Check if the model finely deals with new data and the work process is not broken. 1 week. Input: Integrated model. Output: ready analysis of the model, project finish. Risks: problems with the model. Solution: ask about solution the developers wich have expirience with it.\n",
        "\n",
        "Workload distribution per each team member:\n",
        "\n",
        "Ilia Mitrokhin: full stack and ML developer. Team leader.\n",
        "\n",
        "Alie Ablaeva: ML developer.\n",
        "\n",
        "Anastasia Pichugina: in charge of reports and data preparation.\n",
        "\n",
        "\n",
        "![Table1](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/table1.png)\n",
        "\n",
        "\n",
        "![Table2](https://raw.githubusercontent.com/paket2004/Stock-market-prediction/images/images/table2.png)\n",
        "\n",
        "Gantt Chart Link: https://teamflame.ru/66868eca1229a1f6e892b9b6/p/66868eca1229a1f6e892b9b9/b/66868eca1229a1f6e892b9bb.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
